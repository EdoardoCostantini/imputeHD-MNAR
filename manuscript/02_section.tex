\section{Imputation methods and Algorithms} \label{secMethods}
We will use the following notation: scalars, vectors, and matrices are denoted by italic lowercase, bold lowercase, and bold uppercase letters, respectively.
A scalar belonging to an interval is indicated by $s_1 \in [s_2, s_3]$, while a scalar taking the values in a set is represented as $s_1 \in \{s_2, s_3\}$.

Consider an $n \times p$ data set, $\bm{Z}$, comprising variables $\bm{z}_1$, $\bm{z}_2$, ..., $\bm{z}_p$.
Assume that the first $t$ variables of $\bm{Z}$ have missing values, and that these $t$ variables are the targets of imputation.
Denote the columns of $\bm{Z}$ containing $\bm{z}_1$ to $\bm{z}_t$ as the $n \times t$ matrix, $\bm{T}$.
The remaining $(p-t)$ columns of $\bm{Z}$ contains variables that are not targets of imputation.
These variables constitute a pool of possible \emph{auxiliary} variables that could be used to improve 
the imputation procedure.
Let $\bm{A}$ denote this set of auxiliary variables so that $\bm{Z} = (\bm{T}, \bm{A})$.
For a given $\bm{z}_j$, with $j = (1, ..., p)$, denote its observed and missing components 
as $\bm{z}_{j, obs}$ and $\bm{z}_{j, mis}$, respectively.
Let $\bm{Z}_{-j} = (\bm{z}_1, ..., \bm{z}_{j-1}, \bm{z}_{j+1}, ..., \bm{z}_{p})$ be the collection of $p-1$ variables in 
$\bm{Z}$ excluding $\bm{z}_j$.
Denote by $\bm{Z}_{-j, obs}$ and $\bm{Z}_{-j, mis}$ the components of $\bm{Z}_{-j}$ corresponding to the
data units in $\bm{z}_{j, obs}$ and $\bm{z}_{j, mis}$, respectively.

\subsection{Multiple imputation by chained equations}

Assume that $\bm{Z}$ is the result of $n$ random samples from a multivariate distribution defined by 
an unknown set of parameters $\bm{\theta}$.
The chained equations approach obtains the posterior distribution of $\bm{\theta}$ by sampling iteratively 
from conditional distributions of the form $P(\bm{z}_{1}|\bm{Z}_{-1}, \bm{\theta}_{1})$ $...$ 
$P(\bm{z}_{t}|\bm{Z}_{-t}, \bm{\theta}_{t})$, where $\bm{\theta}_{1}$ $...$ $\bm{\theta}_{t}$ are imputation model 
parameters specific to the conditional distributions of each variable with missing values.

More precisely, the MICE algorithm takes the form of a Gibbs sampler. At the $m$th iteration $(m = 1, ..., M)$, samples are drawn for the $j$th target variable ($j = 1, ..., t$) from the following distributions:
%
	\begin{align}
	\hat{\bm{\theta}}_{j}^{(m)} &\sim 
		p(\bm{\theta}_j | \bm{z}_{j, obs}, \bm{Z}_{-j, obs}^{(m)}) 
		\label{eq_pd}\\
	\bm{z}_{j, mis}^{(m)} &\sim 
		p(\bm{z}_{j, mis} | \bm{Z}_{-j, mis}^{(m)}, \hat{\bm{\theta}}_{j}^{(m)}) 
		\label{eq_ppd},
	\end{align}
%
where $\hat{\bm{\theta}}_{j}^{(m)}$ and $\bm{z}_{j, mis}^{(m)}$ are draws from the parameter's full conditional posterior 
distribution \eqref{eq_pd} and the missing data posterior predictive distribution \eqref{eq_ppd}, respectively.
After convergence, $d$ sets of values are sampled from \eqref{eq_ppd} and used as imputations.
Any substantive model can then be fit to each of the $d$ completed data sets, and the estimates can be pooled using Rubin's rules
\citep{rubin:1987}.

Generally speaking, for each target of imputation, $\bm{z}_j$, the researcher needs to define a set of variables that will be included in $\bm{Z}_{-j}^{(m)}$.
The high-dimensional imputation methods compared in this paper all follow the general MICE framework,
but they differ in the elementary imputation methods they use to define equations \eqref{eq_pd} and \eqref{eq_ppd}.
Each method has a different way of processing the auxiliary variables provided to the 
imputation algorithm, but all of them are designed to support an inclusive strategy while avoiding its usual obstacles.

\subsubsection{MICE with a fixed ridge penalty}
	MICE with a fixed ridge penalty (bridge) uses the Bayesian normal linear model described by \citet[][p. 68, algorithm 3.1]{vanBuuren:2018} as its elementary imputation method (i.e., the model used to define equations \eqref{eq_pd} and \eqref{eq_ppd}).
	In this approach, the sampling of each $\hat{\bm{\theta}}_{j}^{(m)}$ in equation \eqref{eq_pd} relies on inverting the cross-products matrix of $\bm{Z}_{j, obs}^{(m)}$.
	Adding a biasing ridge penalty, $\kappa$, to the diagonal of this cross-products matrix circumvents singularity, and allows
	sampling even when $\bm{Z}_{j, obs}^{(m)}$ is afflicted by collinearity or when $n$ is not 
	substantially larger than $p$.

	The value of $\kappa$ is usually chosen to be close to zero (e.g. $\kappa = 0.0001$), because values larger than $0.1$ 
	may introduce systematic bias \citep[p. 68]{vanBuuren:2018}.
	However, larger values may be necessary to invert the cross-products matrix in certain scenarios.
	In the present work, we choose the value of $\kappa$ by means of cross-validation.

\subsubsection{MICE with Bayesian lasso}
	\cite{zhaoLong:2016} proposed the MICE with Bayesian lasso imputation algorithm (blasso), an MI procedure that uses the Bayesian lasso as its elementary imputation method.
	Bayesian lasso is a regular Bayesian multiple regression model with priors on the slope coefficients 
	that allow interpreting the mode of the slopes' posterior distribution as lasso estimates 
	\citep{parkCasella:2008, hans:2009}.
	Given data with a sample size $n$, consider a dependent variable $\bm{y}$, and a set of predictors $\bm{X}$.
	The Bayesian lasso specification that we used for the blasso imputation algorithm is that specified 
	by \cite{hans:2010}:
%
	\begin{align}
	% Likelihood equation
		p(\bm{y}|\bm{\beta}, \sigma^2, \tau) &= \textrm{N}(\bm{y}|\bm{X\beta}, \sigma^2\bm{I}_n) \label{eqn:dens} \\
	% Prior for regression coefficients
		p(\beta_j|\tau, \sigma^2, \rho) &= 
			(1 - \rho) \delta_0 \beta_j +
			\rho \left( \frac{\tau}{2\sigma} \right) \times 
			\textrm{exp} \left( \frac{-\tau \norm{\beta}_1}{\sigma} \right) \label{eqn:bprior} \\
	% Hyperprior for error variance
		\sigma^2 &\sim \textrm{Inverse-Gamma}(a, b) \label{eqn:sigprior} \\
	% Hyperprior for penalty parameter
		\tau &\sim \textrm{Gamma}(r, s) \label{eqn:tauprior} \\
	% Hyperprior for sparsity parameter
		\rho &\sim \textrm{Beta}(g, h) \label{eqn:rhoprior}
	\end{align}
%	
	Equation \eqref{eqn:dens} represents the density function of a multivariate normal random variable 
	with mean $\bm{X\beta}$ and covariance matrix $\sigma^2\bm{I}_n$ evaluated at $\bm{y}$.
	Equation \eqref{eqn:bprior} represents the expansion of the \cite{parkCasella:2008} double exponential 
	prior developed by \cite{hans:2010} to accommodate the uncertainty regarding model sparsity.
	Finally, equations \eqref{eqn:sigprior} to \eqref{eqn:rhoprior} represent hyper priors for the residual variance, 
	$\sigma^2$, the penalty parameter, $\tau$, and the sparsity parameter, $\rho$.
	Our implementation of blasso imputation replaces equation \eqref{eq_pd} with the blasso model defined by equations \eqref{eqn:dens} to \eqref{eqn:rhoprior} with $\bm{y} = \bm{z}_{j,obs}$ and $\bm{X} = \bm{Z}_{-j,obs}$.

	The R code used to perform the blasso imputation was based on the R Package \emph{blasso} \citep{blasso} and can 
	be found on the main author's GitHub page.
	For a detailed description of the Bayesian lasso MI algorithm in a univariate missing data context see \cite{zhaoLong:2016}.

\subsubsection{Direct use of regularized regression}
	As proposed by \cite{zhaoLong:2016} and \cite{dengEtAl:2016}, frequentist regularized regression can be 
	directly used in a MICE algorithm.
	At iteration $m$, for a target variable $\bm{z}_j$, the Direct Use of Regularized Regression imputation algorithm (DURR) replaces equations \eqref{eq_pd} and \eqref{eq_ppd} with the following two steps:

	\begin{enumerate}

	\item Generate a bootstrap sample $\bm{Z}^{*(m)}$ by sampling with replacement from $\bm{Z}$,
		and train a regularized linear regression model (such as lasso regression) with
		$\bm{z}_{j,obs}^{*(m)}$ as outcome and $\bm{Z}_{-j,obs}^{*(m)}$ as predictors.
		This produces a set of parameter estimates (regression coefficients and error variance),
		$\hat{\bm{\theta}}_{j}^{(m)}$, that can be viewed as a sample from equation \eqref{eq_pd}.

	\item Predict $z_{j,mis}$, based on $\bm{Z}_{-j, mis}$ and $\hat{\bm{\theta}}_{j}^{(m)}$, 
		to obtain draws from the posterior predictive distribution of the missing data as in equation 
		\eqref{eq_ppd}.

	\end{enumerate}

\subsubsection{Indirect use of regularized regression}
	While DURR simultaneously performs model regularization and parameter estimation in equation \eqref{eq_pd}, the Indirect Use of Regularized Regression imputation algorithm (IURR) uses regularized regression exclusively for model trimming. The selected predictors are then used in a standard MI procedure \citep{zhaoLong:2016, dengEtAl:2016}.
	At iteration $m$, the IURR algorithm performs the following steps for each target variable, $\bm{z}_j$:
%
	\begin{enumerate}
%
	\item Fit a linear regression model using a regularized method that does variable selection (e.g., lasso). Take $\bm{z}_{j,obs}$ as the
		dependent variable and $\bm{Z}_{-j,obs}^{(m)}$ as the predictors (unlike DURR, IURR uses the original data, not a bootstrap sample).
		The regression coefficients that are \emph{not} shrunk to 0 define the active 
		set of variables that will be used as predictors in the actual imputation model. \label{varSelectStep}
	
	\item Obtain the maximum likelihood estimates of the regression coefficients and the error variance from the linear
		regression of $\bm{z}_{j,obs}$ onto the active set of predictors defined in step \ref{varSelectStep}. Then, sample new values of these parameters from a multivariate normal distribution
		parameterized by the MLEs\footnote{The sampling notation is the same used by \cite{dengEtAl:2016}.}:
%
		\begin{equation}\label{eq_MLEpd}
		(\hat{\bm{\theta}}_{j}^{(m)}, \hat{\sigma}_{j}^{(m)}) \sim N(\hat{\bm{\theta}}_{MLE}^{(m)}, 
			\hat{\bm{\Sigma}}_{MLE}^{(m)})
		\end{equation}
%
		so that equation \eqref{eq_MLEpd} corresponds to equation \eqref{eq_pd} in the general MICE framework.

	\item Impute $\bm{z}_{j,mis}$ by sampling from the posterior predictive distribution based 
		on $\bm{Z}_{-j,mis}^{(m)}$ and the parameters' posterior draws, $(\hat{\bm{\theta}}_{j}^{(m)}, 
		\hat{\sigma}_{j}^{(m)})$.
%
	\end{enumerate}

\subsubsection{MICE with PCA}
	By extracting principal components (PCs) from the auxiliary variables, it is possible to summarize the information 
	contained in these auxiliaries with just a few components.
	These PCs can then be used as predictors in a standard, low-dimensional application of MICE. The MICE with PCA (MI-PCA) procedure can be summarized as follows:

	\begin{enumerate}

	\item Extract the first PCs that cumulatively explain at most 50\% of the variance 
		in the auxiliary variables, $\bm{A}$, and collect them in a new matrix, $\bm{A}'$;
	\item Replace the auxiliary variables, $\bm{A}$, in $\bm{Z}$ with $\bm{A}'$ to obtain 
		$\bm{Z}' = (\bm{T}, \bm{A}')$;
	\item Use the standard MICE algorithm with the Bayesian normal linear model 
		\citep[p. 68, algorithm 3.1]{vanBuuren:2018} as elementary imputation method to obtain multiply 
		imputed datasets from $\bm{Z}'$.
	\end{enumerate}

	If the auxiliary variables are incomplete, their missing values can be imputed with stochastic SI in a pre-processing step.
	Extracting PCs does not require the estimation of standard errors, so MI is unnecessary and SI suffices.
	The MI-PCA method was inspired by \cite{howardEtAl:2015} and the \emph{PcAux} R package \citep{PcAux} that 
	implements and developed its ideas.
	
\subsubsection{MICE with classification and regression trees}
	MICE with classification and regression trees \citep[MI-CART;][]{burgetteReiter:2010} is a MICE algorithm that uses classification and regression trees (CART) as the elementary imputation method.
	Given an outcome variable $\bm{y}$ and a set of predictors $\bm{X}$, CART is a nonparametric recursive partitioning technique 
	that models the relationship between $\bm{y}$ and $\bm{X}$ by sequentially splitting observations into subsets of units with 
	relatively homogeneous $\bm{y}$ values.
	At every splitting stage, the CART algorithm searches through all variables in $\bm{X}$ to find the best binary 
	partitioning rule to predict $\bm{y}$.
	The resulting collection of binary splits can be visually represented by a decision tree structure where each terminal 
	node (or \emph{leaf}) represents the conditional distribution of $\bm{y}$ for units that satisfy the splitting rules.

%Every observation with a missing value on $z_j$ belongs to a terminal node of this CART model, depending on their values of $\bm{Z}_{-j, mis}^{(m)}$. 
	
	For each $\bm{z}_j$, the $m$th iteration of MI-CART proceeds as follows:
	
	\begin{enumerate}
	\item Train a CART model to predict $\bm{z}_{j, obs}$ from the corresponding $\bm{Z}_{-j, obs}^{(m)}$.
	\item Assign each element of $\bm{z}_{j,mis}$ to a terminal node by applying the splitting rules from the fitted CART model to $\bm{Z}_{-j,mis}$.
	\item Create imputations for each element of $\bm{z}_{j,mis}$ by sampling from the pool of $\bm{z}_{j, obs}$ in the terminal node containing $\bm{z}_{j,mis}$. This procedure corresponds to sampling from the missing data posterior predictive distribution in Equation \eqref{eq_ppd}.
	\end{enumerate}
	
	This approach does not consider uncertainty in the imputation model parameters since the tree structure is not perturbed between iterations. Therefore, MI-CART cannot produce proper imputations in the sense of \citet{rubin:1986}. The implementation of MI-CART used in this paper corresponds to the one presented by \citet[][p. 95, algorithm 1]{dooveEtAl:2014} and the \emph{impute.mice.cart()} function from the \emph{mice} package.

\subsubsection{MICE with random forests}
	MICE with random forests (MI-RF) is a MICE algorithm that uses random forests as the elementary imputation method. The random forest algorithm entails fitting many decision trees (e.g., CART models) to subsamples of the original data. These subsamples are derived by resampling rows with replacement and sampling subsets of columns without replacement. The random forest algorithm results in an ensemble of fitted decision trees that generate a sample of predictions for each outcome value. 
	
	For each $\bm{z}_j$, the $m$th iteration of MI-RF proceeds as follows:
		\begin{enumerate}
	    \item Generate $k$ bootstrap samples from $\bm{Z}_{-j,obs}$.
	    \item Use these bootstrap samples to fit $k$ single trees predicting $\bm{z}_{j,obs}$ from a random subset of the variables in $\bm{Z}_{-j,obs}$.
	    \item Generate a pool $k$ terminal nodes for each element of $\bm{z}_{j,mis}$ by applying the splitting rules from each of the $k$ fitted trees to the appropriate columns of $\bm{Z}_{-j,mis}$.
	    \item Create imputations for each element of $\bm{z}_{j,mis}$ by sampling from the $\bm{z}_{j,obs}$ contained in the pool of terminal nodes defined above.
	\end{enumerate}
	 
	Bootstrapping and random input selection introduce uncertainty regarding the imputation model parameters (i.e., the tree sructure),
	as required by a proper MI procedure.
	For more details on the MI-RF algorithm, see \citet[][algorithm A.1, p. 103]{dooveEtAl:2014}.
	The programming of our implementation of the algorithm was heavily inspired by the \emph{impute.mice.rf()} function in the 
	R package \emph{mice}.

\subsubsection{MICE optimal model}
	When dealing with a large set of possible predictors for the imputation model, a common recommendation in the MI 
	literature is to decide which predictors to include by following three criteria \citep[p. 168]{vanBuuren:2018}:
	\begin{enumerate}

	\item include all variables that are part of the analysis models;
	\item include all variables that are related to the nonresponse;
	\item include all variables that are correlated with the targets of imputation.

	\end{enumerate}

	In practice, researchers can never be sure that the second requirement is entirely met, as there is no way to know exactly 
	which variables are responsible for missingness.
	However, with simulated data, we know which variables are involved in the missing data mechanisms.
	MICE optimal model (MI-OP) is an ideal specification of the MICE algorithm that uses this knowledge to include only the relevant predictors in the imputation models. The imputations were generated using the Bayesian normal linear model as the elementary imputation method.
	
\subsection{Non-MI strategies}

\subsubsection{missForest}
	Most research on high-dimensional imputation has focused on applications for genomics data, where the goal is to prepare large data sets (e.g., DNA microarray data) for high-dimensional
	prediction algorithms, rather than inferential analysis.
	For this reason, a variety of SI methods based on machine learning algorithms have been proposed
	and compared \citep[e.g.,][]{deAndradeSilvaHruschka:2009, stekhovenBuhlmann:2011}.

	In this study, we consider the missForest (missFor) imputation method proposed by \cite{stekhovenBuhlmann:2011},
	which is a popular nonparametric imputation approach that can accommodate for a large number of predictors, 
	can handle missing variables of the mixed data type, and has been robustly implemented in a popular 
	R-package \citep{missForest}.
	The approach consists of an iterative imputation that first trains a random forest on observed values, and then 
	uses the trained random forest to impute the missing values by averaging the predictions from its different trees.
	As a single imputation method we do not expect it to perform well for inferential tasks, at least compared to 
	the MI methods discussed above.

\subsubsection{Complete case analysis}
	By default, most data analysis software either fails in the presence of missing values or defaults to listwise 
	deletion wherein only complete cases are used for the analysis \citep{R:2020, pandas:2020}.
	As the default behavior of most analysis tools, complete cases analysis (CC) remains a popular missing data treatment 
	in the social sciences, despite its well-known flaws (\citeauthor{rubin:1987}, \citeyear{rubin:1987}, p. 8; 
	\citeauthor{vanBuuren:2018}, \citeyear{vanBuuren:2018}, p. 9, \citeauthor{baraldiEnders:2010}, 
	\citeyear{baraldiEnders:2010}).
	Therefore, we include CC as a negative reference point.

\subsubsection{Gold standard}
	Finally, we fit the analysis models directly to the fully observed data before imposing any missing data. In the following, we refer to the results obtained in this fashion as the gold standard (GS). These results represent the counterfactual analysis that would have been performed if there had been no missing data.
