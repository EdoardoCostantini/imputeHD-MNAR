\maketitle
\subsection{Experiment 2: Data with latent structure}

The performance of the selected methods was compared in the presence of a latent structure. It is expected 
that the PcAux method will perform better than the others, given how it extracts components from the data 
that might overlap with the latent constructs. Performances were assessed as in experiment 1, in terms of 
how well the incomplete-data analysis remained statistically valid after missing data was treated. 

\subsubsection{Experimental Conditions}

Three design parameters were varied in the simulation: the proportion of (per variable) missing cases ($pm$),
with levels $\{.1, .3\}$, as in the first experiment, the number of latent variables ($p$), with levels $\{10, 
100\}$, and the size of the factor loadings ($\lambda = {high, low}$). 5 items were generated in all crossed
coditions for each of the latent vairables.

In all conditions, 200 observations were generated ($n = 200$) and 5 items measuring each latent variable 
were generated so that when conducting imputations, the conditions with 100 latent variables resulted 
in a number of potential imputation model auxiliary variables higher than the number of observations
available.

\begin{table}[h]
	\centering
	\caption{Experiment 1 conditions ($n = 200$)}
	\break
	\begin{tabular}{ c c c c }
		Cond & fl & pm & lv \\ 
		\hline
		1 & high & 0.10 & 10 \\ 
		2 & high & 0.10 & 100 \\ 
		3 & high & 0.30 & 10 \\ 
		4 & high & 0.30 & 100 \\ 
		5 & low & 0.10 & 10 \\ 
		6 & low & 0.10 & 100 \\ 
		7 & low & 0.30 & 10 \\ 
		8 & low & 0.30 & 100 \\
		\hline
	\end{tabular}
	\label{table:exp1_conds}
\end{table}

500 data sets were generated for each condition. 

\subsubsection{Data Generation}

A dataset X, with dimensionality $n \times 5\delta$ (with n = number of observations and $\delta$ = number 
of latent variables), was generated based on the following latent variable model:

\begin{equation}
	INSERT MATH HERE
\end{equation}

with $\Phi$, $\Theta$ being the latent variable covariance matrix and items error covairance matrix.
$\lambda$ was sampled from a uniform distribution with bounds .9 and .97, in the conditions with 
high values of the factor loadings, and .5, and .6, in the conditions wiht low values of the factor loadings.
The first set of bounds was chosen on the lines of what has been done in previuos research (see Hastie Elastic 
Net paper), while the second was chosen to mimic scales administred in social survey.
In all of the factorial conditions, 4 latent vairables were highly correlated ($\rho = .6$), another
4 latent vairables were only correlated by $\rho = .3$ among themselves and with the highly correlated latent
variables. The remaining number of latent variables were independently generated around a mean score of 0.
Items were then generated assuming no correlation between their measurment error.

\subsubsection{Missing Data Imposition}

Missing values were imposed on ten items measuring two highly correlated latent vairables (5 items for each).
The probability of missingness was determined by a linear combination of the the other two highly correlated
latente variables.

\subsubsection{Imputation Methods and Analyses Model}

For each simulated data set, imputation was performed according to the same set of imputation methods as in 
experiment 1.

We crossvalidated the ridge parameter used in the bridge imputation method by imputing the dataset under a 
a range of ridge penalties (8 values equally spaced between .1 and 1e-08) and selecting the value that, 
for each condition, granted the lowest average fmi for the analysis model parameters of interest.

In this set up, the "oracle" standard mice imputation ("MI-OP") imputed all items with missing values using only 
the other 19 items corresponding to the 4 most correlated latent vairables. This included all items measuring the 
latent constructs determining the response probability.

Convergence of the imputations was checked through visual examination of trace plots showing the mean imputed values
for each variable at each iteration. In the most complex condition, ($lv = 100, pm = .3$), all methods converged after 
after approximately 20 iterations. In the simulation study we run a single chain of the MI algorithms for 50 
iterations, considering the first 20 as burn-in, and selecting 10 imputed datasets through thinning.
The only exception was 'blasso' which required around 1000 iterations to reach satistifactory stationarity and mixing
of multiple chains of imputation.

Three main types of analysis model were applied: a Confirmatory Factor Analysis to obtain estiamtes of 
the factor loadings was run on the raw data; a saturated model was fitted to the raw data to obtain MLE 
estiamtes of the means, variances, and covariances of the items with missing values. The items measuring
every latent variable were averaged as to obtain a scale score. The saturated model fitted to the raw
data was also fitted on the scales obtained with the imputed values to assess the performance in a context
that is similar to a real survey data application.
