\section{Simulation Studies}

The simulation study was broken up in two separate experiments: (1) the first was used to define a baseline comparison 
of the methods on multivariate normal data in both high and low dimensional conditions; (2) the second was used to 
assess the performance of the methods in the presence of a latent structure, in order to reflect the fundamental
structure of social survey data.

\subsection{Procedure}
	
	To assess the statistical validity of the different imputation methods we have repeated the following steps
	1000 times ($R = 1000$) for each experiment:

	\begin{enumerate}
		\item Data generation - A data matrix $\bm{X}_{n \times p}$ was generated according to an experiment 
			specific model (e.g. multivariate normal model, confirmatory factor analysis).
			The characteristics of the data generating model (e.g. covariance matrix, factor loadings) 
			depend on experimental conditions described below.
		\item Missing data imposition - Missing values were imposed on a given number of target variables
			in $\bm{X}_{n \times p}$, according to some response model.
		\item Imputations - Each method described in section 2 to deal with missing values was used to impute
			NAs.
		\item Analysis - Different analysis models were fitted to the differently treated data.
			Parameters estimates were pooled across the differently imputed datasets for the MI methods and
			stored along with the estimates obtained with single imputation methods and complete case 
			analysis.
	\end{enumerate}

	The $R$ estimates obtained with the Gold Standard approach are averaged and considered as "true" reference
	values of the parameters in the analysis models.
	The $R$ estimates obtained with all other methods are used to obtain performance measures for each imputation 
	method (see below).

	The code to Run the simulation was written in the R statistical programming language (version 4.0.3). 
	All experiments were run using a 2.6 GHz Intel Xeon(R) Gold 6126 processor, 523780 MB of Memory. The
	operating system was Windows Server 2012 R2.

	Computations were run in parallel across the available cores (between 20 and 30). Parallel computing 
	was implemented using the R package 'parallel' and to ensure replicability of the findings seeds were
	set using the method by \cite{lecuyer:2002} implemented in the R package 'rlecuyer'.
	Code to run the studies can be found at [INSERT GITHUB LINK].

%\subsection{Methods: two simulation studies}

\subsubsection{Step 1: Data generations}

	\paragraph{Experiment 1} 
	The $\bm{X}_{n \times p}$ data matrix in step 1 was generated by drawing from a multivariate normal 
	model with a mean vector $\bm{\mu_0}$ of $p$ 0s and a covariance matrix $\bm{\Sigma}_0$, with diagonal 
	elements (variances) equal to 1. 
	The off-diagonal elements of $\bm{\Sigma}_0$ were used to define three blocks of variables: 
	the first five variables were highly correlated among themselves ($\rho = .6$); 
	variables 6 to 10 were slightly correlated with variables in block 1 and among themselves ($\rho = .3$), 
	and all the remaining $p-10$ variables were uncorrelated.

	\paragraph{Experiment 2}
	The observed data $\bm{X}_{n \times p}$ was created based on a Confirmatory Factor Analysis model.
	Each of $l$ latent variables was assumed to be measured by 5 items, for a total of $p = 5 \times l$ 
	number of predictors in $\bm{X}$.
	Values on the observed items for the $i$-th observation were obtained with the following measurement 
	model:

	\begin{equation}
		\bm{x}_i = \bm{\Lambda} \bm{\xi}_{i.} + \bm{\delta}_{i.}
	\end{equation}

	where $\bm{x}_i$ is a vector of $5 \times l$ observed items scores, for observations $i = 1, ..., n$;
	$\bm{\Lambda}$ is the matrix of factor loadings; $\bm{\xi}_{i.}$ is a vector of scores on the
	latent variables for observation $i$; and $\bm{\delta_{i.}}$ is a vector of uncorrelated multivariate 
	normal measurement errors. 
	For notation and model specification the interested reader may refer to [CITE Bollen1989].

	The latent scores in $\bm{\xi}_{i.}$ are sampled from a multivariate normal distribution centered around 
	an $n \times l$ vector of $0s$, and a covariance matrix $\bm{\Psi}_0$, with diagonal elements equal to 1 
	and off-diagonal elements equal to correlation between latent factors. In particular, the first 4 latent 
	variables are highly correlated ($\rho = .6$), the second block of 4 latent variables are somewhat 
	correlated ($\rho = .3$), while the remaining $l-8$ latent variables are uncorrelated.

	The matrix $\bm{\Lambda}$ defines a simple latent structure where each item loads on only 1 factor (5 items 
	for each latent variable).
	Both the item and latent factor variancess are set to 1, $var(x_i) = 1$ and $\Psi_{ii} = 1$, so that 
	the measurement error is defined as $var(\delta) = 1 - \lambda^{2} 1$. 
	This specification allows factor loadings $\lambda_{ij}$, with $i = 1, ..., n$ and $j = 1, ..., l$, to be
	defined as standardized values that range between 0 and 1.
	If all values in $\bm{\Lambda}$ are 0s, there is no latent structure and items are simply drawn from  
	multivariate distribution centered around the item means with covariance matrix $\bm{\Psi}_0$.
	If all values in $\bm{\Lambda}$ are 1s, there is a \emph{prefect} latent structure meaning that items
	exactly measure the latent constructs.
	The exact values for the latent factors are drawn for each repetition from a uniform distribution between
	a lower and upper bound, $b_l$ and $b_u$, that are condition-specific (see below).

\subsubsection{Step 2: Missing data imposition} \label{sub_missing}

	The non-response mechanism was modelled as a logit regression:

	\begin{equation} \label{eqn:rm}
		p(x_t = NA | X) = \bm{\Phi}(\tilde{X}\bm{\theta})
	\end{equation}

	with $x_t$ a variable target of missing data imposition, $\bm{\Phi}(.)$ being the logistic cumulative
	distribution function, $\bm{\tilde{X}}$ the matrix of predictors participating in the missing data mechanism,
	and $\bm{\theta}$ a vector of non-trivial regression coefficients.
	
	An offsetting constant was added to the linear combination $\tilde{X}\bm{\theta}$ to make observations with 
	lower values of $\tilde{X}\bm{\theta}$ have higher chances of having a missing value on the target $x_t$. 
	The offsetting constant was chosen to minimize the difference between a target proportion of missing values 
	and its actual value.

	\paragraph{Experiment 1}
	Six variables were chosen as target of missing data imposition: three variables in the block of 
	highly correlated and three in the block of lowly correlated variables ($x_t$ with $t = 1,2,3,6,7,8$). 
	Item non-response was imposed following equation (\ref{eqn:rm}) with 4 variables included in $\tilde{X}$: two fully 
	observed variables from the highly- and two from the lowly correlated group of variables ($x_r$ with $r = 4,5,9,10$).

	The choice of predictors in $\tilde{X}$ is important to allow imputations under MAR for the imputation methods. 
	The probability of observing a response for a target variable did not depend on the variable itself, to avoid  
	imputation under Missing Not At Random.
	The predictors in $\tilde{X}$ are always provided to the imputation algorithms so that the MAR assumption can
	be met.

	\paragraph{Experiment 2}
	Item non-response was imposed on the 10 items measuring two highly correlated latent variables ($l = 1, 2$) 
	using the other two highly correlated latent variables ($l = 3, 4$) as predictors in response model (\ref{eqn:rm}). 

\subsubsection{Step 3: Imputation}
	
	Imputations are performed according to the 10 methods described in section 2.

\subsubsection{Step 4: Analysis}
	
	\paragraph{Experiment 1}
	The substantive model of interest in experiment 1 is a saturated model that estimates means,
	variances, and covariances of the six variables with missing values.

	\paragraph{Experiment 2}
	The same saturated model is fitted to the data with a latent structure to obtain means, variances, 
	and covariances on the observed items.
	Furthermore, an oracle Confirmatory Factor Analysis was also chosen to see how the factor scores 
	are recovered after imputation.

\subsection{Comparison Criteria} \label{criteria}
	To compare the performance of the different imputation methods, several outcome measures were considered.

	\paragraph{Bias}

	First, we used Percent Relative Bias ($PRB$) and Standardized Bias ($SB$) to quantify the bias introduced by the imputation
	procedure.

	\begin{equation} \label{eqn:prb}
		PRB = \frac{\bar{\hat{\theta}} - \theta}{\theta} \times 100
	\end{equation}

	\begin{equation} \label{eqn:sb}
		SB =  \frac{\bar{\hat{\theta}} - \theta}{SD_{\hat{\theta}}}
	\end{equation}
	
	where $\theta$ represents the reference value (the "true" value) of the focal parameter, 
	$\bar{\hat{\theta}}$ represents its estimate under a given imputation method averaged over 
	the MCMC replications, 
	$SD_{\hat{\theta}}$ represents the empirical standard deviation of $\theta$.

	\paragraph{Confidence Intervals Coverage}
	To assess the integrity of hypothesis testing, the Confidence Interval Coverage of the reference value
	was considered as

	\begin{equation} \label{eqn:cic}
		CIC =  \frac{ \sum_{r=1}^{R} I(\hat{\theta} \in \widehat{CI}_r ) }{R}
	\end{equation}

	where R is the total number of MCMC repetitions, 
	$\theta$ and $\hat{CI}_r$ are, respectively, the estimate and the confidence interval for the focal estimates 
	in a given repetition, 
	and $I(.)$ is the indicator function that returns 1 if the argument is true and 0 otherwise.

	CICs below .9 are generally considered problematic for 95\% confidence intervals \cite[p. 52]{vanBuuren:2018} 
	as they imply inflated Type I error rates.
	A high coverage (e.g., .99) may indicate confidence intervals that are too wide, implying that
	the imputation method leads to more conservative inferential conclusions, and in this sense it is less worrisome 
	than lower than nominal coverage.

	In the present work, we followed \cite{burtonEtAl:2006} and considered as problematic CI coverage rates
	outside of two SEs of the nominal coverage probability (p).
	The standard error of nominal coverage is defined as $SE(p) = \sqrt{p (1-p)/R}$, with $p = .95$.

	An additional measure that can help understanding Confidence Interval Coverages is the width of the confidence
	intervals:

	\begin{equation} \label{eqn:ciw}
		CIW =  \frac{ \sum_{r=1}^{R} [\widehat{CI}_{r,upper} - \widehat{CI}_{r,lower}] }{R}
	\end{equation}

	with $\widehat{CI}_{r,upper}$ and $\widehat{CI}_{r,lower}$ the upper and lower bounds, respectively, 
	of the estimated confidence interval for a parameter estimate in the \emph{r}-th replication.
	CIW is an indicator of the statistical efficacy of the imputation methods.
	There are no rules of thumb for interpreting this measure, but comparing the relative width of 95\%
	Confidence Intervals after imputation with the ones obtained with their Gold Standard value is quite 
	informative.
	In general, for the same confidence interval coverage, imputation methods that have smaller CIW are
	considered more efficient.
	Single imputation methods will tend to provide narrower CIs compared to Gold Standard, while proper 
	multiple imputations will tend to provide wider CIs, reflecting the greater uncertainty regarding a 
	parameter value due to presence of missing values.

	\paragraph{Euclidean Distance}
	Both bias and CIC are computed for individual parameter estimates. 
	When many parameters are involved in the analysis model these measures become cumbersome to compare.
	Hence, we have also used multivariate measures to assess the overall statistical validity of the analysis 
	models of interest after imputation.

	A multi-parameter measure of bias is computed as the Euclidean Distance between the $t$-dimensional vector of
	true model parameters values $\bm{\theta}$ and the vector of MCMC-average after-imputation estimates 
	$\bar{\hat{\bm{\theta}}}$:

	\begin{equation} \label{eqn:ed}
		d_{bias}(\bm{\theta},\bar{\hat{\bm{\theta}}}) =  
			\sqrt{ \sum_{i=1}^{t} (\bm{\theta}_i - \hat{\bm{\theta}}_i)^2 }
	\end{equation}
	
	$\bar{\hat{\bm{\theta}}}$ is computed as $\frac{ \sum_{r=1}^{R} \hat{\bm{\theta}}_r }{R}$,
	where $\hat{\bm{\theta}}_r$ is the vector of the model parameters estimates in the \emph{r}-th repetition.

	A multi-parameter measure of Confidence Interval Coverage is computed as Euclidean Distance between a 
	$t$-dimensional vector ($\bm{CIC}^{n}$) of nominal coverage values (.95) and the vector of MCMC actual
	coverages for all model parameters $\bm{CIC}^{a}$:

	\begin{equation} \label{eqn:ed}
		d_{CIC}(\bm{CIC}^{n}, \bm{CIC}^{a}) =  
			\sqrt{ \sum_{i=1}^{t} (\bm{CIC}^{n}_i - \bm{CIC}^{a}_i)^2 }
	\end{equation}
	
	$\bm{CIC}^{a}_i$ is computed as in equation \ref{eqn:cic}.

\subsection{Conditions}
	
	Data generation and missing data imposition was defined by design factors specific to each experiment.
	The procedure outlined above was run for each of the conditions in table \ref{table:condSum}.

	\paragraph{Experiment 1}
	Two experimental factors were considered: $p$, the number of columns in the dataset, which 
	are all fed to the imputation algorithms, and $pm$, the target proportion of \emph{per} variable missing cases.
	The sample size $n$ was set to 200 in all conditions.

	\paragraph{Experiment 2}
	The dimensionality of the data was controlled based on the number of latent variables $l$.
	Two values were used for this factor: 10 and 100. 
	In all conditions, 5 items were generated as measures for each latent variable, making conditions with $l = 10$ 
	low dimensional conditions, with 50 total predictors and a constant sample size of 200 observations, and 
	conditions with $l = 100$ high dimensional ones, with data matrices of dimensionality $200 \times 500$.
	The proportion of missing values was defined again as a fixed experimental factor with two levels: .1 or .3.

	In experiment 2, we have also defined the latent structure factor loadings $\lambda_{ij}$ as a 2-level random experimental factor.
	The data generation step used factor loadings drawn from a uniform distribution defined between either .5 and .6, or 
	or .9 and .97.

\begin{table}[t]
   \begin{center}
        \begin{tabular}{ c c c c c c }
		% Column names
		\textbf{condition} & \textbf{n} & \textbf{p} & \textbf{l} & \textbf{pm} & \textbf{$\lambda$ range} \\ 
		\hline

		% Group name
		\rowcolor{Gray}
 		\multicolumn{6}{c}{Experiment 1} \\
		\hline

		% Row 1
		1 & 200 & 50 & 0 & .1 & - \\
		2 & 200 & 500 & 0 & .1 & - \\
		3 & 200 & 50 & 0 & .3 & - \\
		4 & 200 & 500 & 0 & .3 & - \\
	\hline
		& & & & & \\ 

		% Group Name
		\rowcolor{Gray}
 		\multicolumn{6}{c}{Experiment 2} \\
		\hline

  1 & 200 & 50 &  10 &  0.1 & [.9, .97] \\
  2 & 200 & 500 & 100 & 0.1 & [.9, .97] \\
  3 & 200 & 50 &  10 &  0.3 & [.9, .97] \\
  4 & 200 & 500 & 100 & 0.3 & [.9, .97] \\
  5 & 200 & 50 &  10 &  0.1 & [.5, .6]  \\
  6 & 200 & 500 & 100 & 0.1 & [.5, .6]  \\
  7 & 200 & 50 &  10 &  0.3 & [.5, .6]  \\
  8 & 200 & 500 & 100 & 0.3 & [.5, .6]  \\

		\hline
        \end{tabular}
    \end{center}
\caption{Summary of conditions for experiment 1 and 2}
\label{table:condSum}
\end{table}


\FloatBarrier % stops table:condSum leaving its own section

\subsection{Results}

\subsubsection{Experiment 1}

	\paragraph{Saturated Model} Figure \ref{fig:exp1bias} reports the Percentage Relative Bias computed 
	for each parameter estimate in the saturated model described above: item means, variances, and covariances for
	the six variables with missing values.

	Focusing first on the means (leftmost column), all methods achieve a bias that is smaller than the 10\% threshold 
	for all item means, in all conditions.
	Looking at relative performances, we can see how IURR and MI-PCA always result in the smallest
	estimation bias. 
	Bridge competes with these two methods in the low dimensional conditions (rows 1 and 3), but it 
	does lose ground in the higher dimensional conditions (2 and 4).

	Moving to the item variances (central column) we notice that IURR, Blasso, and the tree based methods are 
	giving the lowest item variance biases across all conditions, even in the most difficult one (row 4).
	Somewhat surprisingly, directly using regularized regression within the imputation models (DURR) leads
	to a bias larger than 10\% in the high dimensional condition with high proportion of missing values.
	
	% Think about this more
\textcolor{cyan}{
	It is interesting to note that while, the single imputation method, missForest, leads to extreme negative bias 
	(above 10\%) for the item variances, MI-PCA overestimates the variance of the variables with missing values.
	As a single imputation method, missForsest tends to reduce the variance of the imputations while MI-PCA reflects
	a lot of uncertainty regarding the imputations.
	While not ideal, the latter behaviour is certainly less alarming than the former.
}
	
	Finally, the third column in table \ref{fig:exp1bias} shows the estimation bias for the 15
	covariances between items with missing values.
	As covariances depend on two variables, recovering the correct estimates after imputation is inherently 
	more difficult than with means and variances.
	This explains the generally worse performances reported in the figure: MI-PCA is the only method with
	all covariances PRB lower than 10\% in the high dimensional conditions.
	Indirect Use of Regularized Regression (IURR) performs noticeably better than all other methods, but it 
	seem to struggle with a high bias for the majority of the 15 covariances in condition 4.

	Figure \ref{fig:exp1cir} reports the Confidence Interval Coverage computed for each parameter.
	The pattern of performances is quite similar to that described by bias with MI-PCA and IURR 
	outperforming most other methods in most conditions.
	This suggests that MI-PCA and IURR are the imputation methods doing the best job at preserving
	the integrity of hypothesis testing performed on the treated data.

	However, there are a few exceptions and peculiarities to note: 
	\begin{itemize}
	\item MI-PCA is the only method that does not undercover means and covariances in the most challenging condition
		(row 4);
	\item compared to all other methods, MI-PCA tends toward over- rather than under-coverage, and it 
	does so to a lesser degree;
	\item while the bias for item variances obtained with IURR was quite small in condition 4, the method seems 
	to undercover the true values of this type of parameter, suggesting confidence intervals that are bit smaller
	than they should.
	\end{itemize}

\begin{figure}
	\includegraphics[width=\textwidth]{../../output/graphs/exp1_bias.pdf}
\caption{Percent Relative Bias (PRB) for the means, variances, and covariances broken 
	down by method. 
	Each row represents a different condition. 
	Each column is dedicated to a different parameter type.}
\label{fig:exp1bias}
\end{figure}

\begin{figure}
	\includegraphics[width=\textwidth]{../../output/graphs/exp1_CI.pdf}
\caption{Confidence Interval Coverage (CIR) for the means, variances, and covariances broken 
	down by method. 
	Each row represents a different condition. 
	Each column is dedicated to a different parameter type.}
\label{fig:exp1cir}
\end{figure}
	
\FloatBarrier % stops fig:exp1cir to leave its section

\subsubsection{Experiment 2}

\paragraph{Saturated Model}

	Results form the first experiments held mostly constant in experiment 2, indicating that the presence and 
	strength of a latent structure does not perturb the methods' relative performances.

	Figure \ref{fig:exp2bias} reports the bias of the Saturated Model parameter estimates for the first
	four conditions of experiment 2 (high factor loadings). 
	For both item variances and covariances PRB is reported, while for the item means we reported the SB. 
	Items were generated around a mean of 0 making the computation of PRB for this parameter meaningless.

	The least biased estimates for means and variances are obtained with IURR, in all conditions.
	Imputing missing values with the MI-PCA approach also grants low biases in all conditions.
	Bridge is also performing quite well with the exception of covariance estimates in condition 4.

	In agreement with what was found in experiment 1, the MI-PCA approach is the one resulting in the lowest
	bias for all covariance estimates in all conditions.
	Surprisingly, the bias for the item variances that afflicted MI-PCA in the multivariate-normal
	set up disappears when the latent structure is strong (factor loadings larger than .9).

	Figure \ref{fig:exp2cir} shows results for the confidence interval coverage in experiment 2.
	When factor loadings are high, conditions 1 to 4, we see that all multiple imputation
	methods lead to acceptable coverage for means and variances, in the conditions with low proportion 
	of missing values, no matter the dimensionality of the data.
	For both item means and variances, confidence intervals coverage is approximately within .93 and .97 
	for al methods. 
	As the proportion of missing values increases we see a general deterioration in CIR performances, with IURR
	and MI-PCA still showing the most contained deviations from the target value.
	MI-PCA tends to include the true parameter values more than it should (over-coverage), 
	while most other methods show sings of under-coverage.

	Given the large positive biases obtained by all methods for the covariances of the observed items,
	it comes to no surprise that most methods lead to under-coverage of these parameters in all
	conditions in experiment 2. 
	MI-PCA is again the only exception providing acceptable coverage for all covariances.

	The same pattern can be seen in the conditions with lower factor loadings (reported in appendix). 
	However, in condition 8, the MI-PCA approach also leads to extreme under-coverage of variances 
	(as do most other methods) and it tends again to over-estimate the item variances (PRBs > 20\%).

\begin{figure}
	\includegraphics[width=\textwidth]{../../output/graphs/exp2_semR_bias_sd_14.pdf}
\caption{Bias estimation for the means (SB), variances and covariances (PRB) for condition 1 to 4.
	Each column is dedicated to a different parameter type.}
\label{fig:exp2bias}
\end{figure}

\begin{figure}
	\includegraphics[width=\textwidth]{../../output/graphs/exp2_semR_ci_14.pdf}
\caption{Confidence Interval Coverage (CIC) for the means, variances, and covariances for condition 1 to 4.
	Each column is dedicated to a different parameter type.}
\label{fig:exp2cir}
\end{figure}

\FloatBarrier % stops fig:exp2cir to leave its section

\paragraph{Confirmatory Factor Analysis}

	Figures \ref{fig:exp2fl14} and \ref{fig:exp2fl58} show the PRB values for all the factor loadings estimated by
	the Confirmatory Factor Analysis described above. 
	Most MI-Methods are able to provide acceptably biased estimates for these parameters in 
	all conditions except the ones with a large proportion of missing values and high 
	dimensional input data matrix (condition 4).

	IURR and MI-PCA are again the two top performers giving virtually unbiased estimates
	of the factor loadings in all conditions.
	However, MI-PCA outperforms IURR when factor loadings are low, maintaining inconsequential 
	biases even when data is high-dimensional and the proportion of missing values is high.

\begin{figure}
	\includegraphics[width=\textwidth]{../../output/graphs/exp2_CFA_lambda_BPR_14.pdf}
\caption{Percent Relative Bias (PRB) for the factor loadings conditions 1 to 4.}
\label{fig:exp2fl14}
\end{figure}

\begin{figure}
	\includegraphics[width=\textwidth]{../../output/graphs/exp2_CFA_lambda_BPR_58.pdf}
	\caption{Percent Relative Bias (PRB) for the factor loadings conditions 5 to 8 (wrong 
	labels right now).}
\label{fig:exp2fl58}
\end{figure}

\FloatBarrier % stops fig:exp2fl to leave its section

