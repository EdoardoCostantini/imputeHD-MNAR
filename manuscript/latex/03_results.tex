\maketitle
\section{Results}

\subsection{Bias}

Simulation results show that all multiple imputation methods perform well, in terms of relative bias in percent, in condition 1, 
the low-dimensional benchmark setting: the size of the bias is negligible for all methods ($PBR < 10\%$). MI-CART 
and MI-RANF are the only exceptions, exhibiting bias in percent around and above the 10\% threshold.

As dimensionality increases (condition 2), deterioration of performances is found for most approaches. 
However, IURR and MI-PCA maintain great performances with PBR well below 10\%, for all parameters. Blasso
and DURR show slightly worst performances in terms of covariance bias (up to 10\% in PBR), while maintaining negligible bias 
for all means and variances. Bridge exhibits a drastic reduction in performance, in particular with substantially biased 
variances, on par with complete case analysis results.

In condition 4, the larger proportion of missing values does not dramatically change relative performances.
IURR and MI-PCA are still exhibiting the lower biases, Blasso and DURR slack behind a little, and tree-based 
methods are still underperforming. However, MI-PCA manifests substantial bias of the variance estimates, while 
PBR for covariances and means remains well below 10\%. At the same time, IURR starts to show considerable 
bias in the covariance estimates. Similarly, Multiple Imputation through Bayesian Blasso maintains extremely 
low bias for both means and variances, while displaying substantial covariances bias.

%% INSERT TABLE

\begin{table}[h]

 \centering

 \resizebox{0.75\textwidth}{!}{\begin{minipage}{\textwidth}
 \centering

 \caption{Euclidean Distances between vectors of reference and estimated parameters grouped by 
 			type of statistic ($n = 200$)}
 \break

 \begin{tabular}{lcccccccccc}
  Cond 	& DURR & IURR & bridge & blasso & MI PCA & MI CART & MI RF & MI OP & missFor & CC \\ 
 \hline

 \multicolumn{11}{c}{\textbf{All parameters}} \\
 \hline
  p = 50, pm = .1 & 0.38 & 0.05 & 0.62 & 0.47 & 0.18 & 0.62 & 0.96 & 0.09 & 1.48 & 2.53 \\ 
  p = 50, pm = .3 & 1.02 & 0.14 & 2.80 & 1.41 & 0.76 & 1.62 & 2.48 & 0.39 & 3.88 & 4.65 \\ 
  p = 500, pm = .1 & 0.91 & 0.30 & 6.24 & 0.72 & 0.78 & 0.89 & 1.30 & 0.09 & 2.09 & 2.52 \\ 
  p = 500, pm = .3 & 2.70 & 0.95 & 6.58 & 2.09 & 3.64 & 2.32 & 3.21 & 0.34 & 5.29 & 4.73 \\  
 \hline

 \multicolumn{11}{c}{\textbf{Means}} \\
 \hline
  p = 50, pm = .1 & 0.04 & 0.01 & 0.00 & 0.06 & 0.01 & 0.07 & 0.12 & 0.01 & 0.10 & 1.31 \\ 
  p = 50, pm = .3 & 0.12 & 0.04 & 0.03 & 0.22 & 0.06 & 0.23 & 0.37 & 0.03 & 0.29 & 3.05 \\ 
  p = 500, pm = .1 & 0.08 & 0.04 & 0.10 & 0.09 & 0.04 & 0.11 & 0.16 & 0.01 & 0.20 & 1.30 \\ 
  p = 500, pm = .3 & 0.28 & 0.14 & 0.39 & 0.33 & 0.16 & 0.35 & 0.51 & 0.04 & 0.58 & 3.04 \\ 
 \hline

 \multicolumn{11}{c}{\textbf{Variances}} \\
 \hline
  p = 50, pm = .1 & 0.25 & 0.02 & 0.62 & 0.09 & 0.17 & 0.19 & 0.25 & 0.08 & 1.20 & 1.22 \\ 
  p = 50, pm = .3 & 0.69 & 0.06 & 2.80 & 0.20 & 0.74 & 0.45 & 0.56 & 0.37 & 3.33 & 2.02 \\ 
  p = 500, pm = .1 & 0.66 & 0.14 & 6.19 & 0.13 & 0.76 & 0.21 & 0.27 & 0.07 & 1.40 & 1.23 \\ 
  p = 500, pm = .3 & 2.05 & 0.47 & 6.13 & 0.30 & 3.62 & 0.52 & 0.60 & 0.31 & 3.92 & 2.08 \\ 
 \hline

  \multicolumn{11}{c}{\textbf{Covariances}} \\
 \hline
  p = 50, pm = .1 & 0.29 & 0.05 & 0.03 & 0.45 & 0.05 & 0.59 & 0.93 & 0.04 & 0.86 & 1.78 \\ 
  p = 50, pm = .3 & 0.74 & 0.13 & 0.11 & 1.37 & 0.16 & 1.54 & 2.39 & 0.12 & 1.97 & 2.87 \\ 
  p = 500, pm = .1 & 0.62 & 0.27 & 0.75 & 0.70 & 0.14 & 0.86 & 1.26 & 0.06 & 1.54 & 1.76 \\ 
  p = 500, pm = .3 & 1.74 & 0.81 & 2.36 & 2.04 & 0.31 & 2.23 & 3.11 & 0.14 & 3.51 & 2.96 \\  
 \hline

 \end{tabular}
 \label{table:exp1_conds} 
 \end{minipage}}

\end{table}

%% END TABLE

Euclidean distances reported in table \ref{table:exp1_conds} supplement 
the results based on the Percent Relative Bias:
\begin{itemize}
	\item using high dimensional single imputation methods ('missFor'), the distance between the vector of estimated 
	parameters and the reference vector is many times larger than that of any other imputation method, and, apart from 
	the means, the results match the poor inferential performances of Complete Case analysis.
	\item IURR and MI-PCA outperform all other methods, providing the vectors of parameter estimates 
	closest to the reference values, for all sets of parameters. However, the deteriorated performances of MI-PCA
	and IURR in condition 4 are quite evident in the estimation of variances and covariances, respectively.
	\item blasso performances are overall comparable to MI-PCA and IURR but show greater deterioration of performances 
	due to larger proportion of missing values for covariances.
\iffalse % COMMENTED OUT
	However, in condition 4, the euclidean distance 
	between the reference vector of covariances and the IURR vector of estimates is twice the ED between the MI-PCA
	vector of estimates and the reference.
\fi
	\item the use of regression trees remains quite unsatisfactory, especially when it comes to the bias of covariances.
\end{itemize}

\subsection{Confidence Intervals} 

As for the confidence interval coverage of the reference values, the performance pattern of the approaches is similar
to that of bias, with IURR and MI-PCA maintaining coverage rates closer to nominal levels than all the other methods. 

All of the high-dimensional multiple imputation methods considered perform equally well in the low and high dimensional 
context. In both condition 1 and 2, CIs cover the reference values in approximately 90-95\% of the simulated runs, and, 
for most methods, the coverages do not differ at all between the two conditions.

It is only in condition 3 and 4 that under-coverage of the 95\% CI becomes a real concern. 
Keeping constant the dimensionality of the data (comparing condition 1 and 3, and 2 and 4), a larger $pm$ results in CI 
coverage from the 90-95\% range to 65-80\% range for all approaches, except IURR and MI-PCA. However, in condition 4, 
IURR and MI-PCA start showing sings of under-coverage ($CI_{cov}$ between 85-90 \%) and over-coverage ( $CI_{cov}$ between 
95-98\%), respectively.





